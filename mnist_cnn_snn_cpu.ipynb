{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing MNIST dataset and creating train and test loder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  #torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
    "  torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor()\n",
    "                             ])),\n",
    "  batch_size=batch_size_train,pin_memory = True, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  #torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
    "  torchvision.datasets.MNIST('.', train=False, download=True,\n",
    "\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor()\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, pin_memory = True,shuffle=True)\n",
    "\n",
    "#torchvision.transforms.Normalize(                                 (0.1307,), (0.3081,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "example_data.shape\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.mnist = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, kernel_size=3),\n",
    "            nn.BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(),\n",
    "            nn.Conv2d(16, 20, kernel_size=5),\n",
    "            nn.BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(320, 50),\n",
    "            nn.BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 10),\n",
    "            nn.LogSoftmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mnist(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_cpu = Net()\n",
    "network = network_cpu\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    network.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append(\n",
    "                (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "            torch.save(network.state_dict(), 'model.pth')\n",
    "            torch.save(optimizer.state_dict(), 'optimizer.pth')\n",
    "def test():\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = network(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            #test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            #correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "            correct += pred.cpu().eq(target.data.view_as(pred.cpu())).sum()\n",
    "            test_loss /= len(test_loader.dataset)\n",
    "            test_losses.append(test_loss)\n",
    "            print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "                test_loss, correct, len(test_loader.dataset),\n",
    "                100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.474852\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.227726\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.242449\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.154800\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 1.993896\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 1.935152\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 1.805560\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 1.773948\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 1.668432\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 1.698252\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.588651\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 1.484778\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.571228\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 1.424471\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.361089\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.291594\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.318745\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.162842\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.192407\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 1.278494\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.029237\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 1.097293\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.992710\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 1.008139\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.940846\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.080373\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.903618\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.896669\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.802280\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.777982\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.957047\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.687075\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.718032\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.710734\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.616636\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.798112\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.705225\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.672696\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.688209\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.639701\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.597255\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.703042\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.592922\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.458514\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.542989\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.441046\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.612037\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.443201\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.488150\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.409936\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.468026\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.441504\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.530970\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.447995\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.421684\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.425683\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.515815\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.493986\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.611130\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.636966\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.483891\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.482763\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.435551\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.438383\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.492487\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.443119\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.454905\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.497443\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.450828\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.386009\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.390408\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.338842\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.380999\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.374103\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.427525\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.437195\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.482993\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.303309\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.476084\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.474422\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.344505\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.345475\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.560899\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.369864\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.479977\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.274141\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.231512\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.333514\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.453547\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.522970\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.245919\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.333121\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.334619\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.251822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0161, Accuracy: 960/10000 (10%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0192, Accuracy: 1919/10000 (19%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0192, Accuracy: 2877/10000 (29%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0188, Accuracy: 3834/10000 (38%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0199, Accuracy: 4784/10000 (48%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0181, Accuracy: 5735/10000 (57%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0194, Accuracy: 6690/10000 (67%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0193, Accuracy: 7644/10000 (76%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0182, Accuracy: 8602/10000 (86%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0186, Accuracy: 9560/10000 (96%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.305366\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.407905\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.290866\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.192601\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.347749\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.495357\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.267087\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.297759\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.326936\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.314262\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.269756\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.289230\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.371542\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.398174\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.237257\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.243217\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.331546\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.275434\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.252374\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.316154\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.357058\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.289710\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.356419\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.268626\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.388064\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.376199\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.296011\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.418507\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.216381\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.366598\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.228327\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.270558\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.188035\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.237438\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.338370\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.218476\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.284567\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.367856\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.437795\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.180826\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.121085\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.379696\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.315579\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.284597\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.236592\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.214073\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.194259\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.129023\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.336467\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.232614\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.287360\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.262065\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.239077\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.176153\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.373242\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.251211\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.207578\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.237251\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.162911\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.236510\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.188659\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.248354\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.398495\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.236988\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.477109\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.251902\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.201461\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.232898\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.241881\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.371283\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.300799\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.198087\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.237940\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.161344\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.334959\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.132329\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.169797\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.113590\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.274238\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.304529\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.295067\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.197793\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.178324\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.280921\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.361746\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.246677\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.342503\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.266744\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.109630\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.296537\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.413926\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.152158\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.293108\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.172089\n",
      "\n",
      "Test set: Avg. loss: 0.0096, Accuracy: 977/10000 (10%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0100, Accuracy: 1949/10000 (19%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0099, Accuracy: 2924/10000 (29%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0117, Accuracy: 3887/10000 (39%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0097, Accuracy: 4861/10000 (49%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0096, Accuracy: 5832/10000 (58%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0107, Accuracy: 6799/10000 (68%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0107, Accuracy: 7773/10000 (78%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0110, Accuracy: 8747/10000 (87%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0116, Accuracy: 9710/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.300831\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.269897\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.245830\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.244355\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.270592\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.131305\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.248987\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.316991\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.174091\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.140587\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.166179\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.183578\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.206675\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.239107\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.177108\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.101655\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.179831\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.182139\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.157178\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.302610\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.233208\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.272282\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.226027\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.162724\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.270368\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.320903\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.136508\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.128714\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.163448\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.131297\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.316271\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.265362\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.154998\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.135837\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.149991\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.180382\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.514401\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.281027\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.126883\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.289050\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.217395\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.204024\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.150259\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.198918\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.140340\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.158037\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.165778\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.217411\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.268696\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.126641\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.197039\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.198275\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.250159\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.548673\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.190736\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.182738\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.133463\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.121166\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.211828\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.386591\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.218426\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.170625\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.323379\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.275009\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.201624\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.099942\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.314622\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.298883\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.109727\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.140659\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.193395\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.151439\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.274267\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.142428\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.187536\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.196390\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.160565\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.145254\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.057444\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.248306\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.116449\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.264769\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.309224\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.202362\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.198262\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.151190\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.276208\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.221499\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.290246\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.285805\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.237305\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.082181\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.135852\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.225182\n",
      "\n",
      "Test set: Avg. loss: 0.0098, Accuracy: 968/10000 (10%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0069, Accuracy: 1953/10000 (20%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0110, Accuracy: 2922/10000 (29%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0087, Accuracy: 3897/10000 (39%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0090, Accuracy: 4870/10000 (49%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0070, Accuracy: 5853/10000 (59%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0074, Accuracy: 6831/10000 (68%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0077, Accuracy: 7810/10000 (78%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0069, Accuracy: 8791/10000 (88%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0084, Accuracy: 9768/10000 (98%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.182657\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.152151\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.121171\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.085122\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.257012\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.225128\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.134241\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.187192\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.224857\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.227266\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.347661\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.144858\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.147146\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.166552\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.205713\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.197425\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.079065\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.290882\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.208848\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.205662\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.089322\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.122204\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.141832\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.215969\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.179932\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.195079\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.306392\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.147372\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.199093\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.091905\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.109903\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.153592\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.090935\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.199392\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.294346\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.152650\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.118891\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.121310\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.199964\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.220484\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.164111\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.104393\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.272721\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.298638\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.210093\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.142660\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.261256\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.140648\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.126166\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.212146\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.318543\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.208296\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.183371\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.134400\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.139765\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.228089\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.196673\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.209329\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.130154\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.194404\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.075083\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.227586\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.265272\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.262788\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.151918\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.080944\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.178002\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.163549\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.312846\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.268950\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.107454\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.083061\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.250016\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.135204\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.411084\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.130436\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.091887\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.144661\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.195373\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.216412\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.220666\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.054073\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.120760\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.204990\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.255200\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.144155\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.079109\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.244389\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.136343\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.234893\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.185053\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.089163\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.202886\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.190737\n",
      "\n",
      "Test set: Avg. loss: 0.0059, Accuracy: 984/10000 (10%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0050, Accuracy: 1974/10000 (20%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0058, Accuracy: 2961/10000 (30%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0088, Accuracy: 3933/10000 (39%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0091, Accuracy: 4905/10000 (49%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0068, Accuracy: 5886/10000 (59%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0050, Accuracy: 6874/10000 (69%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0069, Accuracy: 7850/10000 (78%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0078, Accuracy: 8827/10000 (88%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0065, Accuracy: 9805/10000 (98%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.287896\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.097785\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.285896\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.126443\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.132909\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.216788\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.127158\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.164301\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.120060\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.101423\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.107730\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.193414\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.181310\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.096517\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.122734\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.094377\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.112821\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.218800\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.187652\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.107669\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.248217\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.189865\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.223552\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.163004\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.243937\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.142182\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.080055\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.218882\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.117266\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.119019\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.293482\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.111395\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.233174\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.194831\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.137334\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.257663\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.244981\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.188001\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.192226\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.119732\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.147121\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.226862\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.227601\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.112084\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.075091\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.211282\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.198569\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.198881\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.098640\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.212444\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.110566\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.110278\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.115241\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.140127\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.173493\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.176974\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.136679\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.145807\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.092623\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.163457\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.236192\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.155045\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.087137\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.169052\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.050821\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.059341\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.268513\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.137278\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.111073\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.256639\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.133382\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.179254\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.107355\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.126902\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.104879\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.196200\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.164665\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.143226\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.172818\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.084786\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.196898\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.146846\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.183056\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.174028\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.144365\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.098907\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.165980\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.126436\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.064572\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.275445\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.139942\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.174543\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.142436\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.063583\n",
      "\n",
      "Test set: Avg. loss: 0.0051, Accuracy: 984/10000 (10%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0080, Accuracy: 1959/10000 (20%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0073, Accuracy: 2940/10000 (29%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0048, Accuracy: 3928/10000 (39%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0071, Accuracy: 4907/10000 (49%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0053, Accuracy: 5892/10000 (59%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0050, Accuracy: 6877/10000 (69%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0048, Accuracy: 7864/10000 (79%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0056, Accuracy: 8848/10000 (88%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0058, Accuracy: 9826/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Network training\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Loding the weights of the network trained above\n",
    "## This is useful for those who don't want to train the network afresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (mnist): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout2d(p=0.5, inplace=False)\n",
      "    (7): Conv2d(16, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (8): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout2d(p=0.5, inplace=False)\n",
      "    (12): Flatten()\n",
      "    (13): Linear(in_features=320, out_features=50, bias=True)\n",
      "    (14): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=50, out_features=10, bias=True)\n",
      "    (17): LogSoftmax()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "saved_weights = torch.load('model.pth')\n",
    "\n",
    "saved_model = Net()\n",
    "saved_model = saved_model\n",
    "\n",
    "saved_model.load_state_dict(saved_weights)\n",
    "saved_model.eval()\n",
    "print(saved_model)\n",
    "\n",
    "#input_image = cv2.imread('temp_folder/img_137.jpg',0)/255#example_data[2][0]\n",
    "#input_tensor = torch.tensor(input_image[np.newaxis,np.newaxis,:,:],dtype=torch.float32)\n",
    "#print(input_tensor)\n",
    "#output = saved_model(input_tensor.cuda())\n",
    "#print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4731e+01, -1.4589e+01, -1.4156e+01, -5.1974e-05, -1.4648e+01,\n",
      "         -9.9626e+00, -1.5805e+01, -1.3336e+01, -1.4254e+01, -1.4655e+01]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "input_image = cv2.imread('temp_folder/img_137.jpg',0)/255#example_data[2][0]\n",
    "input_tensor = torch.tensor(input_image[np.newaxis,np.newaxis,:,:],dtype=torch.float32)\n",
    "#print(input_tensor)\n",
    "output = saved_model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conv_layer_idx = [0,2,7]\n",
    "relu_layer_idx = [1,5,10,15,16]\n",
    "fc_layer_idx = [13,16]\n",
    "batch_norm2d_idx = [3,8]\n",
    "batch_norm1d_idx = [14]\n",
    "max_pool_idx = [4,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_batch_norm2d(conv_layer_idx, bn2d_idx):\n",
    "    eps =1e-05\n",
    "    conv_weight = saved_weights['mnist.'+str(conv_layer_idx)+'.weight']\n",
    "    conv_bias = saved_weights['mnist.'+str(conv_layer_idx)+'.bias']\n",
    "    bn2d_gamma = saved_weights['mnist.'+str(bn2d_idx)+'.weight']\n",
    "    bn2d_beta = saved_weights['mnist.'+str(bn2d_idx)+'.bias']\n",
    "    \n",
    "    bn2d_mean = saved_weights['mnist.'+str(bn2d_idx)+'.running_mean']\n",
    "    bn2d_var = saved_weights['mnist.'+str(bn2d_idx)+'.running_var']\n",
    "    bn2d_var = torch.sqrt(bn2d_var + eps)\n",
    "    print(conv_weight.shape)\n",
    "\n",
    "    conv_weight = conv_weight.transpose(0,3)\n",
    "    print(conv_weight.shape)\n",
    "    #gamma = bn.weight\n",
    "    #beta = bn.bias\n",
    "    #mean = bn.running_mean\n",
    "    #var = bn.running_var\n",
    "    #eps =1e-05\n",
    "\n",
    "    #var_sqrt = torch.sqrt(var + eps)\n",
    "\n",
    "    #w = (self.weight * gamma.reshape(self.out_channels, 1, 1, 1)) / var_sqrt.reshape(self.out_channels, 1,1, 1)\n",
    "    #b = ((self.bias - mean) * gamma) / var_sqrt + beta\n",
    "\n",
    "    output_conv_weight = torch.mul(conv_weight, torch.div(bn2d_gamma, bn2d_var))\n",
    "    #print(output_conv_weight.shape)\n",
    "    output_conv_weight = output_conv_weight.transpose(0,3)\n",
    "    conv_weight = conv_weight.transpose(0,3)\n",
    "    output_conv_bias = torch.add(torch.mul((conv_bias-bn2d_mean), torch.div(bn2d_gamma,bn2d_var)),bn2d_beta)\n",
    "    return output_conv_weight, output_conv_bias#, #output_conv_weight, \n",
    "\n",
    "def merge_batch_norm1d(fc_layer_idx, bn1d_idx):\n",
    "    eps =1e-05\n",
    "\n",
    "    fc_weight = saved_weights['mnist.'+str(fc_layer_idx)+'.weight']\n",
    "    fc_bias = saved_weights['mnist.'+str(fc_layer_idx)+'.bias']\n",
    "    bn1d_gamma = saved_weights['mnist.'+str(bn1d_idx)+'.weight']\n",
    "    bn1d_beta = saved_weights['mnist.'+str(bn1d_idx)+'.bias']\n",
    "    bn1d_mean = saved_weights['mnist.'+str(bn1d_idx)+'.running_mean']\n",
    "    bn1d_var = saved_weights['mnist.'+str(bn1d_idx)+'.running_var']\n",
    "    bn1d_var = torch.sqrt(bn1d_var + eps)\n",
    "    fc_weight = fc_weight.transpose(0,1)\n",
    "    output_fc_weight = torch.mul(fc_weight, torch.div(bn1d_gamma, bn1d_var))\n",
    "    fc_weight = fc_weight.transpose(0,1)\n",
    "    output_fc_weight = output_fc_weight.transpose(0,1)\n",
    "    output_fc_bias = torch.add(torch.mul((fc_bias-bn1d_mean), torch.div(bn1d_gamma,bn1d_var)),bn1d_beta)\n",
    "    return output_fc_weight, output_fc_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_list_fn(layers_list,data_loader,model):\n",
    "    tensor_list = []\n",
    "    output_dict = {}\n",
    "    count =0\n",
    "    for batch_idx, (data, _) in enumerate(data_loader):\n",
    "        max_output_list =[]\n",
    "        x = data.cpu()\n",
    "        for idx,seq in enumerate(model):#.mnist):\n",
    "            x = seq(x)\n",
    "            if idx in layers_list:\n",
    "                max_output_list.append(x) #intermediate_output#(torch.flatten(x)))\n",
    "        #del data_cuda\n",
    "        #torch.cuda.empty_cache()\n",
    "        iter = 0\n",
    "        if count == 0:\n",
    "            tensor_list.append(data)\n",
    "        else:\n",
    "            tensor_list[iter] = torch.cat((tensor_list[iter],data),0)\n",
    "        iter+=1   \n",
    "        for idx in layers_list:\n",
    "            if count == 0:\n",
    "                tensor_list.append(max_output_list[iter-1])\n",
    "            else:\n",
    "                tensor_list[iter] = torch.cat((tensor_list[iter],max_output_list[iter-1]),0)\n",
    "            iter+=1\n",
    "        \n",
    "        count +=1\n",
    "    tensor_list[0] = torch.flatten(tensor_list[0]).detach().numpy()\n",
    "    output_dict['input'] = tensor_list[0]\n",
    "    iter =1\n",
    "    for idx in layers_list:\n",
    "        tensor_list[iter] = torch.flatten(tensor_list[iter]).detach().numpy()\n",
    "        output_dict[str(idx)] = tensor_list[iter]\n",
    "        iter+=1\n",
    "\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "relu_output_dict = output_list_fn(relu_layer_idx,train_loader,saved_model.mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 percentile 0.9960784316062927\n",
      "99.9 percentile 1.0\n",
      "99.99 percentile 1.0\n",
      "100 percentile 1.0\n",
      "99 percentile 0.6415862441062927\n",
      "99.9 percentile 0.8213360310792979\n",
      "99.99 percentile 0.8799995183944702\n",
      "100 percentile 1.0280272960662842\n",
      "99 percentile 2.909743309020996\n",
      "99.9 percentile 3.5448575034141783\n",
      "99.99 percentile 4.173173904848042\n",
      "100 percentile 5.5232133865356445\n",
      "99 percentile 2.401043438911442\n",
      "99.9 percentile 2.914788252830519\n",
      "99.99 percentile 3.219420201945156\n",
      "100 percentile 4.071824073791504\n",
      "99 percentile 2.990603151321407\n",
      "99.9 percentile 3.7520537772178715\n",
      "99.99 percentile 4.271087199783274\n",
      "100 percentile 4.964450359344482\n",
      "99 percentile 10.894921483993532\n",
      "99.9 percentile 12.508128539085407\n",
      "99.99 percentile 13.514982291221582\n",
      "100 percentile 14.34607982635498\n"
     ]
    }
   ],
   "source": [
    "# print percentiles\n",
    "relu_layers = [1,5,10,15,16]\n",
    "percentile_99 = np.percentile(relu_output_dict['input'],99)\n",
    "percentile_99_9 = np.percentile(relu_output_dict['input'],99.9)\n",
    "percentile_99_99 = np.percentile(relu_output_dict['input'],99.99)\n",
    "percentile_100 = np.percentile(relu_output_dict['input'],100)\n",
    "print('99 percentile', percentile_99)\n",
    "print('99.9 percentile', percentile_99_9)\n",
    "print('99.99 percentile', percentile_99_99)\n",
    "print('100 percentile', percentile_100)\n",
    "percentile_dict = {}\n",
    "percentile_dict['layer_input_99'] = percentile_99\n",
    "percentile_dict['layer_input_99_9'] = percentile_99_9\n",
    "percentile_dict['layer_input_99_99'] = percentile_99_99\n",
    "percentile_dict['layer_input_100'] = percentile_100\n",
    "for idx in relu_layers:\n",
    "    percentile_99 = np.percentile(relu_output_dict[str(idx)],99)\n",
    "    percentile_99_9 = np.percentile(relu_output_dict[str(idx)],99.9)\n",
    "    percentile_99_99 = np.percentile(relu_output_dict[str(idx)],99.99)\n",
    "    percentile_100 = np.percentile(relu_output_dict[str(idx)],100)\n",
    "    percentile_dict['layer_'+str(idx)+'_99'] = percentile_99\n",
    "    percentile_dict['layer_'+str(idx)+'_99_9'] = percentile_99_9\n",
    "    percentile_dict['layer_'+str(idx)+'_99_99'] = percentile_99_99\n",
    "    percentile_dict['layer_'+str(idx)+'_100'] = percentile_100\n",
    "\n",
    "    print('99 percentile', percentile_99)\n",
    "    print('99.9 percentile', percentile_99_9)\n",
    "    print('99.99 percentile', percentile_99_99)\n",
    "\n",
    "    print('100 percentile', percentile_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_weight_bias(weight,bias,l1_out,l2_out):\n",
    "    weight = torch.mul(weight,l1_out/l2_out)\n",
    "    bias = torch.div(bias,l2_out)\n",
    "    return weight,bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 8, 3, 3])\n",
      "torch.Size([3, 8, 3, 16])\n",
      "torch.Size([20, 16, 5, 5])\n",
      "torch.Size([5, 16, 5, 20])\n"
     ]
    }
   ],
   "source": [
    "conv1_weight = saved_weights['mnist.0.weight']\n",
    "conv1_bias = saved_weights['mnist.0.bias']\n",
    "\n",
    "conv2_weight, conv2_bias = merge_batch_norm2d(2,3)\n",
    "conv3_weight, conv3_bias = merge_batch_norm2d(7,8)\n",
    "fc1_weight,fc1_bias = merge_batch_norm1d(13,14)\n",
    "fc2_weight = saved_weights['mnist.16.weight']\n",
    "fc2_bias = saved_weights['mnist.16.bias']\n",
    "\n",
    "snn_conv1_weight,snn_conv1_bias = normalize_weight_bias(conv1_weight,conv1_bias,percentile_dict['layer_input_99_9'],percentile_dict['layer_1_99_9'])\n",
    "snn_conv2_weight,snn_conv2_bias = normalize_weight_bias(conv2_weight,conv2_bias,percentile_dict['layer_1_99_9'],percentile_dict['layer_5_99_9'])\n",
    "snn_conv3_weight,snn_conv3_bias = normalize_weight_bias(conv3_weight,conv3_bias,percentile_dict['layer_5_99_9'],percentile_dict['layer_10_99_9'])\n",
    "snn_fc1_weight,snn_fc1_bias = normalize_weight_bias(fc1_weight,fc1_bias,percentile_dict['layer_10_99_9'],percentile_dict['layer_15_99_9'])\n",
    "snn_fc2_weight,snn_fc2_bias = normalize_weight_bias(fc2_weight,fc2_bias,percentile_dict['layer_15_99_9'],percentile_dict['layer_16_99_9'])\n",
    "\n",
    "\n",
    "\n",
    "#print(snn_conv1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer_idx = [0,2,7]\n",
    "relu_layer_idx = [1,5,10,15,16]\n",
    "fc_layer_idx = [13,16]\n",
    "batch_norm2d_idx = [3,8]\n",
    "batch_norm1d_idx = [14]\n",
    "max_pool_idx = [4,9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 2000\n",
    "test_batch_size = 1000\n",
    "snn_train_loader = torch.utils.data.DataLoader(\n",
    "  #torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
    "  torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor()\n",
    "                             ])),\n",
    "  batch_size=train_batch_size,pin_memory = False, shuffle=True)\n",
    "\n",
    "snn_test_loader = torch.utils.data.DataLoader(\n",
    "  #torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
    "  torchvision.datasets.MNIST('.', train=False, download=True,\n",
    "\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor()\n",
    "                             ])),\n",
    "  batch_size=test_batch_size, pin_memory = False,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "train_size =0\n",
    "for batch_idx, (data, target) in enumerate(snn_train_loader):\n",
    "    train_size += data.shape[0]\n",
    "print(train_size)\n",
    "\n",
    "test_size =0\n",
    "for batch_idx, (data, target) in enumerate(snn_test_loader):\n",
    "    test_size += data.shape[0]\n",
    "print(test_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data gen: Generate data and analyze how many time steps on an average is needed for someone to confirm the output\n",
    "#spikenet_fc_1 = spiking_fc(1,snn_fc1_weight,snn_fc1_bias,1).cuda()\n",
    "#spikenet_fc_2 = spiking_fc(1,snn_fc2_weight,snn_fc2_bias,1).cuda()\n",
    "#spike_frame = torch.zeros_like(input_tensor)\n",
    "#spike_pot = torch.zeros_like(input_tensor)\n",
    "sp1_shape = snn_conv1_weight.shape\n",
    "spikenet_1 = nn.Conv2d(sp1_shape[0],sp1_shape[1],kernel_size=sp1_shape[2])\n",
    "spikenet_1.weight = nn.Parameter(snn_conv1_weight)\n",
    "spikenet_1.bias = nn.Parameter(snn_conv1_bias)\n",
    "sp2_shape = snn_conv2_weight.shape\n",
    "spikenet_2 = nn.Conv2d(sp2_shape[0],sp2_shape[1],kernel_size=sp2_shape[2])\n",
    "spikenet_2.weight = nn.Parameter(snn_conv2_weight)\n",
    "spikenet_2.bias = nn.Parameter(snn_conv2_bias)\n",
    "sp3_shape = snn_conv3_weight.shape\n",
    "spikenet_3 = nn.Conv2d(sp3_shape[0],sp3_shape[1],kernel_size=sp3_shape[2])\n",
    "spikenet_3.weight = nn.Parameter(snn_conv3_weight)\n",
    "spikenet_3.bias = nn.Parameter(snn_conv3_bias)\n",
    "sp_fc1_shape = snn_fc1_weight.shape\n",
    "spikenet_fc_1 = nn.Linear(sp_fc1_shape[0],sp_fc1_shape[1])\n",
    "spikenet_fc_1.weight = nn.Parameter(snn_fc1_weight)\n",
    "spikenet_fc_1.bias = nn.Parameter(snn_fc1_bias)\n",
    "sp_fc2_shape = snn_fc2_weight.shape\n",
    "spikenet_fc_2 = nn.Linear(sp_fc2_shape[0],sp_fc2_shape[1])\n",
    "spikenet_fc_2.weight = nn.Parameter(snn_fc2_weight)\n",
    "spikenet_fc_2.bias = nn.Parameter(snn_fc2_bias)\n",
    "max_pool2 = nn.MaxPool2d(2, stride=2)\n",
    "max_pool3 = nn.MaxPool2d(2, stride=2)\n",
    "number_of_timesteps = 500 #1000\n",
    "train_output_expected = np.zeros((train_size,1))\n",
    "train_output_array = np.zeros((train_size,number_of_timesteps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Running SNN on train data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_timesteps = 500\n",
    "test_output_expected = np.zeros((test_size,1))\n",
    "test_output_array = np.zeros((test_size,number_of_timesteps))\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(snn_train_loader):\n",
    "    #x = data np.zeros((1000,1000))\n",
    "    start = batch_idx*train_batch_size\n",
    "    end = batch_idx*train_batch_size + data.shape[0]\n",
    "    test_output_expected[start:end,0] = target.detach().numpy()\n",
    "    data = data\n",
    "    #print(data.shape)\n",
    "    spike_pot = torch.zeros_like(data).detach()\n",
    "    sp1_out = torch.zeros((data.shape[0],8,26,26)).detach()\n",
    "    sp2_out = torch.zeros((data.shape[0],16,24,24)).detach()\n",
    "    sp3_out = torch.zeros((data.shape[0],20,8,8)).detach()\n",
    "    sp_fc1_out = torch.zeros((data.shape[0],50)).detach()\n",
    "    sp_fc2_out = torch.zeros((data.shape[0],10)).detach()\n",
    "\n",
    "    \n",
    "    for i in range(number_of_timesteps):\n",
    "        print(i)\n",
    "        first_frame = True if i ==0 else False\n",
    "        #x = spike_frame_mnist.reset_by_subtraction(input_tensor,first_frame)\n",
    "            \n",
    "        #spike_pot = torch.add(spike_pot, data)\n",
    "        spike_pot.add_(data)\n",
    "\n",
    "        spike_frame = torch.gt(spike_pot,1.0)*1.0\n",
    "        spike_pot.sub_(spike_frame)\n",
    "        sp1_out.add_(spikenet_1(spike_frame))\n",
    "        sp2_in = torch.gt(sp1_out,1.0)*1.0\n",
    "        sp1_out.sub_(sp2_in)\n",
    "        sp2_out.add_(spikenet_2(sp2_in))\n",
    "        sp3_in = torch.gt(sp2_out,1.0)*1.0\n",
    "        sp2_out.sub_(sp3_in)\n",
    "        sp3_in_red = max_pool2(sp3_in)\n",
    "        sp3_out.add_(spikenet_3(sp3_in_red))\n",
    "        sp4_in = torch.gt(sp3_out,1.0)*1.0\n",
    "        sp3_out.sub_(sp4_in)\n",
    "        sp4_in_red = max_pool3(sp4_in)\n",
    "        sp4_in_red_flat = torch.flatten(sp4_in_red,start_dim=1)\n",
    "        sp_fc1_out.add_(spikenet_fc_1(sp4_in_red_flat))\n",
    "        sp_fc2_in = torch.gt(sp_fc1_out,1.0)*1.0\n",
    "        sp_fc1_out.sub_(sp_fc2_in)\n",
    "        sp_fc2_out.add_(spikenet_fc_2(sp_fc2_in))\n",
    "        sof_pot = F.softmax(sp_fc2_out)\n",
    "        max_numbers = torch.argmax(sof_pot,dim=1)\n",
    "        test_output_array[start:end,i] = max_numbers.cpu().detach().numpy()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Running SNN on test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1000\n",
      "batch id  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 2000\n",
      "batch id  1\n",
      "2000 3000\n",
      "batch id  2\n",
      "3000 4000\n",
      "batch id  3\n",
      "4000 5000\n",
      "batch id  4\n",
      "5000 6000\n",
      "batch id  5\n",
      "6000 7000\n",
      "batch id  6\n",
      "7000 8000\n",
      "batch id  7\n",
      "8000 9000\n",
      "batch id  8\n",
      "9000 10000\n",
      "batch id  9\n"
     ]
    }
   ],
   "source": [
    "### RAM PLEASE LOOK AT THIS CODE\n",
    "\n",
    "number_of_timesteps = 400\n",
    "\n",
    "test_output_expected = np.zeros((test_size,1))\n",
    "test_output_array = np.zeros((test_size,number_of_timesteps))\n",
    "for batch_idx, (data, target) in enumerate(snn_test_loader):\n",
    "    start = batch_idx*test_batch_size\n",
    "    end = batch_idx*test_batch_size + data.shape[0]\n",
    "    print(start,end)\n",
    "    test_output_expected[start:end,0] = target.detach().numpy()\n",
    "    spike_pot = torch.zeros_like(data).detach()\n",
    "    sp1_np = np.zeros((data.shape[0],8,26,26))\n",
    "    sp1_out = torch.tensor(sp1_np, requires_grad=False, dtype=torch.float32)\n",
    "    sp2_np = np.zeros((data.shape[0],16,24,24))\n",
    "    sp2_out = torch.tensor(sp2_np, requires_grad=False, dtype=torch.float32)\n",
    "    sp3_np = np.zeros((data.shape[0],20,8,8))\n",
    "    sp3_out = torch.tensor(sp3_np, requires_grad=False, dtype=torch.float32)\n",
    "    sp_fc1_np = np.zeros((data.shape[0],50))\n",
    "    sp_fc1_out = torch.tensor(sp_fc1_np, requires_grad=False, dtype=torch.float32)\n",
    "    sp_fc2_np = np.zeros((data.shape[0],10))\n",
    "    sp_fc2_out = torch.tensor(sp_fc2_np, requires_grad=False, dtype=torch.float32)\n",
    "    print('batch id ', batch_idx)\n",
    "    for i in range(number_of_timesteps):\n",
    "        #print(i)\n",
    "        first_frame = True if i ==0 else False\n",
    "        spike_pot.add_(data)\n",
    "\n",
    "        spike_frame = torch.gt(spike_pot,1.0)*1.0 #F.threshold_(spike_pot,1,0).sign() #\n",
    "        spike_pot.sub_(spike_frame)\n",
    "        sp1_out.add_(spikenet_1(spike_frame).detach())\n",
    "        sp2_in = torch.gt(sp1_out,1.0)*1.0 #F.threshold_(sp1_out, 1, 0) #\n",
    "        sp1_out.sub_(sp2_in)\n",
    "        sp2_out.add_(spikenet_2(sp2_in).detach())\n",
    "        sp3_in = torch.gt(sp2_out,1.0)*1.0 #F.threshold_(sp2_out, 1, 0).sign() # \n",
    "        sp2_out.sub_(sp3_in)\n",
    "        sp3_in_red = max_pool2(sp3_in)\n",
    "        sp3_out.add_(spikenet_3(sp3_in_red).detach())\n",
    "        sp4_in = torch.gt(sp3_out,1.0)*1.0 #F.threshold_(sp3_out, 1, 0).sign() #\n",
    "        sp3_out.sub_(sp4_in)\n",
    "        sp4_in_red = max_pool3(sp4_in)\n",
    "        sp4_in_red_flat = torch.flatten(sp4_in_red,start_dim=1)\n",
    "        sp_fc1_out.add_(spikenet_fc_1(sp4_in_red_flat).detach())\n",
    "        sp_fc2_in = torch.gt(sp_fc1_out,1.0)*1.0 #F.threshold_(sp_fc1_out, 1, 0).sign() #\n",
    "        sp_fc1_out.sub_(sp_fc2_in)\n",
    "        sp_fc2_out.add_(spikenet_fc_2(sp_fc2_in).detach())\n",
    "        sof_pot = F.softmax(sp_fc2_out).detach()\n",
    "        max_numbers = torch.argmax(sof_pot,dim=1)\n",
    "        test_output_array[start:end,i] = max_numbers.cpu().detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_over_time = np.zeros(test_output_array.shape[1])\n",
    "total_samples = test_output_array.shape[0]\n",
    "from scipy import stats\n",
    "for idx in range(test_output_array.shape[1]):\n",
    "    part_of_output = test_output_array[:,0:idx+1]\n",
    "    max_of_part = stats.mode(part_of_output,axis=1)[0]\n",
    "    #print(max_of_part)\n",
    "    percentage = np.sum(np.equal(max_of_part ,test_output_expected)*1.0)\n",
    "    #print(percentage)\n",
    "    accuracy_over_time[idx] = percentage*100/total_samples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.1  12.68 12.33 29.7  52.37 91.8  96.2  96.8  97.4  97.62 97.88 97.88\n",
      " 97.94 97.95 97.98 98.   98.02 98.03 98.05 98.05 98.06 98.05 98.1  98.08\n",
      " 98.07 98.05 98.05 98.06 98.1  98.11 98.12 98.14 98.15 98.16 98.18 98.2\n",
      " 98.2  98.21 98.22 98.21 98.21 98.21 98.2  98.2  98.19 98.19 98.2  98.2\n",
      " 98.2  98.19 98.2  98.19 98.19 98.19 98.19 98.18 98.17 98.17 98.19 98.17\n",
      " 98.17 98.17 98.17 98.15 98.16 98.16 98.16 98.15 98.16 98.16 98.16 98.15\n",
      " 98.15 98.14 98.15 98.15 98.14 98.15 98.13 98.13 98.12 98.12 98.11 98.11\n",
      " 98.11 98.11 98.11 98.11 98.12 98.12 98.12 98.12 98.12 98.12 98.12 98.12\n",
      " 98.12 98.12 98.12 98.12 98.12 98.12 98.12 98.12 98.12 98.12 98.12 98.12\n",
      " 98.12 98.12 98.12 98.12 98.12 98.12 98.12 98.12 98.12 98.12 98.12 98.12\n",
      " 98.12 98.12 98.12 98.12 98.12 98.12 98.12 98.12 98.12 98.13 98.13 98.13\n",
      " 98.13 98.13 98.13 98.13 98.13 98.12 98.11 98.11 98.11 98.11 98.11 98.11\n",
      " 98.11 98.11 98.11 98.11 98.11 98.11 98.11 98.11 98.11 98.11 98.11 98.11\n",
      " 98.11 98.11 98.11 98.11 98.11 98.1  98.1  98.1  98.1  98.09 98.09 98.09\n",
      " 98.09 98.09 98.09 98.09 98.09 98.08 98.07 98.07 98.07 98.07 98.07 98.07\n",
      " 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07\n",
      " 98.06 98.06 98.06 98.06 98.06 98.06 98.06 98.06 98.06 98.06 98.06 98.06\n",
      " 98.06 98.06 98.06 98.06 98.06 98.06 98.07 98.07 98.07 98.07 98.07 98.07\n",
      " 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07\n",
      " 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07\n",
      " 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07\n",
      " 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07\n",
      " 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07\n",
      " 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07\n",
      " 98.07 98.08 98.07 98.07 98.07 98.07 98.07 98.08 98.08 98.08 98.08 98.08\n",
      " 98.08 98.08 98.08 98.08 98.08 98.08 98.08 98.08 98.08 98.08 98.08 98.08\n",
      " 98.08 98.08 98.08 98.08 98.07 98.07 98.07 98.07 98.08 98.08 98.08 98.08\n",
      " 98.08 98.08 98.08 98.08 98.08 98.08 98.08 98.08 98.08 98.08 98.08 98.08\n",
      " 98.08 98.08 98.08 98.08 98.08 98.08 98.08 98.08 98.08 98.08 98.08 98.08\n",
      " 98.08 98.08 98.08 98.08 98.08 98.08 98.08 98.08 98.08 98.08 98.08 98.08\n",
      " 98.08 98.08 98.08 98.08 98.08 98.08 98.08 98.08 98.07 98.07 98.07 98.07\n",
      " 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07\n",
      " 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07 98.07\n",
      " 98.07 98.07 98.07 98.07]\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_over_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(output_array[199])\n",
    "print(train_output_array.shape)\n",
    "print(train_output_expected.shape)\n",
    "print(test_output_array.shape)\n",
    "print(test_output_expected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class compute_analysis():\n",
    "    def __init__(self,inp_ch, out_ch, padding='valid', filter_size=(3,3)):\n",
    "        #super.init(compute_analysis,self).__init__()\n",
    "        self.inp_ch = inp_ch\n",
    "        self.out_ch = out_ch\n",
    "        self.filter_size = filter_size\n",
    "        self.padding = padding\n",
    "        self.total_number_of_conv_ops = 0\n",
    "        self.single_conv_ops =0 \n",
    "        self.shape = None\n",
    "        self.output_shape = np.zeros(2)\n",
    "        self.first_ifmap = 1\n",
    "        self.ifmap_sparsity_list = [] #torch.zeros(1)\n",
    "        self.number_of_zero_ifmaps = 0\n",
    "        self.number_of_nonzero_addtions =0\n",
    "        self.layer_dict ={}\n",
    "        self.sparsity_tensor = None\n",
    "        self.conv_counter = nn.Conv2d(inp_ch,out_ch,kernel_size=filter_size).cuda()\n",
    "        self.conv_weight = torch.zeros((out_ch,inp_ch,*filter_size),dtype= torch.float32)+1.0\n",
    "        self.conv_bias = torch.zeros(out_ch)\n",
    "\n",
    "        self.conv_counter.weight = nn.Parameter(self.conv_weight.cuda())\n",
    "        self.conv_counter.bias = nn.Parameter(self.conv_bias.cuda())\n",
    "        \n",
    "    ## This will measure sparsity over input images    \n",
    "    def crude_conv_analysis(self,input_frame,first_frame):\n",
    "        if first_frame == True:\n",
    "            self.shape = input_frame.shape\n",
    "            shape = self.shape\n",
    "            number_of_inp_pix = shape[0]*shape[1]*shape[2]*shape[3]\n",
    "            if self.padding == 'valid':\n",
    "                self.output_shape[0] = int(shape[2]-self.filter_size[0]+1)\n",
    "                self.output_shape[1] = int(shape[3]-self.filter_size[1]+1)\n",
    "            else:\n",
    "                self.output_shape[0] = int(shape[2])\n",
    "                self.output_shape[1] = int(shape[3])\n",
    "            \n",
    "            self.total_number_of_conv_ops = shape[0]*(self.output_shape[0])*(self.output_shape[1])*self.out_ch*self.filter_size[0]*self.filter_size[1]*shape[1]#self.inp_ch\n",
    "            self.single_conv_ops = (self.output_shape[0])*(self.output_shape[1])*self.out_ch*self.filter_size[0]*self.filter_size[1]*self.inp_ch\n",
    "            number_of_mult_in_conv_per_op = self.filter_size[0]*self.filter_size[1]*self.inp_ch\n",
    "            for i in range(shape[0]):\n",
    "                for j in range(shape[1]):\n",
    "                    for id1 in range(int(self.output_shape[0])):\n",
    "                        for id2 in range(int(self.output_shape[1])):\n",
    "                            if self.first_ifmap == 1:\n",
    "                                number_of_on_fields = torch.sum(input_frame[i,j,id1:id1+self.filter_size[0],id2:id2+self.filter_size[1]])\n",
    "                                self.number_of_nonzero_addtions += number_of_on_fields*self.out_ch\n",
    "                                if number_of_on_fields ==0:\n",
    "                                    self.number_of_zero_ifmaps+=1\n",
    "                                #print('before',self.ifmap_sparsity_list,torch.div(number_of_on_fields,number_of_mult_in_conv_per_op))\n",
    "                                self.ifmap_sparsity_list.append(torch.div(number_of_on_fields,number_of_mult_in_conv_per_op).detach().numpy().item(0))\n",
    "                                #print('after',self.ifmap_sparsity_list)\n",
    "\n",
    "                                #self.ifmap_sparsity_list.append(torch.div(number_of_on_fields,number_of_mult_in_conv_per_op))\n",
    "                                #print(torch.div(number_of_on_fields,number_of_mult_in_conv_per_op))#self.ifmap_sparsity_list)\n",
    "                                self.first_ifmap = 0\n",
    "                            else:\n",
    "                                number_of_on_fields = torch.sum(input_frame[i,j,id1:id1+self.filter_size[0],id2:id2+self.filter_size[1]])\n",
    "                                self.number_of_nonzero_addtions += number_of_on_fields*self.out_ch\n",
    "                                if number_of_on_fields ==0:\n",
    "                                    self.number_of_zero_ifmaps+=1\n",
    "                                #print(torch.div(number_of_on_fields,number_of_mult_in_conv_per_op))#self.ifmap_sparsity_list)\n",
    "                                #print('before',self.ifmap_sparsity_list)\n",
    "\n",
    "                                self.ifmap_sparsity_list.append(torch.div(number_of_on_fields,number_of_mult_in_conv_per_op).detach().numpy().item(0))\n",
    "                                #print('after',self.ifmap_sparsity_list)\n",
    "\n",
    "                                #self.ifmap_sparsity_list = torch.cat((self.ifmap_sparsity_list[0],torch.div(number_of_on_fields,number_of_mult_in_conv_per_op)),0)\n",
    "        \n",
    "        else:\n",
    "            self.shape = input_frame.shape\n",
    "            shape = self.shape\n",
    "            number_of_inp_pix = shape[0]*shape[1]*shape[2]*shape[3]\n",
    "\n",
    "            self.total_number_of_conv_ops += shape[0]*(self.output_shape[0])*(self.output_shape[1])*self.out_ch*self.filter_size[0]*self.filter_size[1]*shape[1]#self.inp_ch\n",
    "            number_of_mult_in_conv_per_op = self.filter_size[0]*self.filter_size[1]*self.inp_ch\n",
    "            for i in range(shape[0]):\n",
    "                for j in range(shape[1]):\n",
    "                    for id1 in range(int(self.output_shape[0])):\n",
    "                        for id2 in range(int(self.output_shape[1])):\n",
    "                            number_of_on_fields = torch.sum(input_frame[i,j,id1:id1+self.filter_size[0],id2:id2+self.filter_size[1]])\n",
    "                            self.number_of_nonzero_addtions += number_of_on_fields*self.out_ch\n",
    "                            if number_of_on_fields ==0:\n",
    "                                self.number_of_zero_ifmaps+=1\n",
    "                            #print(torch.div(number_of_on_fields,number_of_mult_in_conv_per_op))#self.ifmap_sparsity_list)\n",
    "                            #self.ifmap_sparsity_list = torch.cat((self.ifmap_sparsity_list,torch.div(number_of_on_fields,number_of_mult_in_conv_per_op)),0)\n",
    "                            self.ifmap_sparsity_list.append(torch.div(number_of_on_fields,number_of_mult_in_conv_per_op).detach().numpy().item(0))\n",
    "\n",
    "                            #self.ifmap_sparsity_list = torch.cat((self.ifmap_sparsity_list[0],torch.div(number_of_on_fields,number_of_mult_in_conv_per_op)),0)\n",
    "                            \n",
    "        \n",
    "\n",
    "            \n",
    "        return 0\n",
    "    def conv_analysis(self,input_frame,first_frame):\n",
    "        if first_frame == True:\n",
    "            self.shape = input_frame.shape\n",
    "            shape = self.shape\n",
    "            number_of_inp_pix = shape[0]*shape[1]*shape[2]*shape[3]\n",
    "            if self.padding == 'valid':\n",
    "                self.output_shape[0] = int(shape[2]-self.filter_size[0]+1)\n",
    "                self.output_shape[1] = int(shape[3]-self.filter_size[1]+1)\n",
    "            else:\n",
    "                self.output_shape[0] = int(shape[2])\n",
    "                self.output_shape[1] = int(shape[3])\n",
    "\n",
    "            self.total_number_of_conv_ops = shape[0]*(self.output_shape[0])*(self.output_shape[1])*self.out_ch*self.filter_size[0]*self.filter_size[1]*shape[1]#self.inp_ch\n",
    "            self.single_conv_ops = (self.output_shape[0])*(self.output_shape[1])*self.out_ch*self.filter_size[0]*self.filter_size[1]*self.inp_ch\n",
    "            number_of_mult_in_conv_per_op = self.filter_size[0]*self.filter_size[1]*self.inp_ch*self.out_ch\n",
    "            #self.hist_bins = np.arange(number_of_mult_in_conv_per_op+1)\n",
    "            number_of_on_fields = self.conv_counter(input_frame).detach().cpu()\n",
    "            self.sparsity_tensor= torch.histc(number_of_on_fields,bins = number_of_mult_in_conv_per_op+1,max = number_of_mult_in_conv_per_op,min=0)\n",
    "            self.number_of_nonzero_addtions += torch.sum(number_of_on_fields).detach().item()\n",
    "            self.number_of_zero_ifmaps += torch.sum(torch.eq(number_of_on_fields,0)*1.0).detach().item()\n",
    "            #self.sparsity_tensor = number_of_on_fields\n",
    "            #self.sparsity_tensor = torch.div(number_of_on_fields,number_of_mult_in_conv_per_op).detach()\n",
    "        \n",
    "        else:\n",
    "            self.shape = input_frame.shape\n",
    "            shape = self.shape\n",
    "            number_of_inp_pix = shape[0]*shape[1]*shape[2]*shape[3]\n",
    "\n",
    "            self.total_number_of_conv_ops += shape[0]*(self.output_shape[0])*(self.output_shape[1])*self.out_ch*self.filter_size[0]*self.filter_size[1]*shape[1]#self.inp_ch\n",
    "            number_of_mult_in_conv_per_op = self.filter_size[0]*self.filter_size[1]*self.inp_ch*self.out_ch\n",
    "            number_of_on_fields = self.conv_counter(input_frame).detach().cpu()\n",
    "            self.number_of_nonzero_addtions += torch.sum(number_of_on_fields).detach().item()\n",
    "            self.number_of_zero_ifmaps += torch.sum(torch.eq(number_of_on_fields,0)*1.0).detach().item()\n",
    "            self.sparsity_tensor.add_(torch.histc(number_of_on_fields,bins = number_of_mult_in_conv_per_op+1,max = number_of_mult_in_conv_per_op,min=0))\n",
    "\n",
    "            #self.sparsity_tensor = torch.cat((self.sparsity_tensor,number_of_on_fields),0)\n",
    "            #self.sparsity_tensor = torch.cat((self.sparsity_tensor,torch.div(number_of_on_fields,number_of_mult_in_conv_per_op).detach()),0)\n",
    "\n",
    "            \n",
    "        return 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    def summary(self):\n",
    "        self.layer_dict  = {'total_pure_ann_ops':self.total_number_of_conv_ops, 'single_ann_ops':self.single_conv_ops, 'ifmap_sparsity_list':self.ifmap_sparsity_list,'number_of_zero_ifmaps':self.number_of_zero_ifmaps,'total_nonzero_ops':self.number_of_nonzero_addtions,'sparsity_tensor':torch.flatten(self.sparsity_tensor)}\n",
    "        return self.layer_dict\n",
    "        \n",
    "        \n",
    "    def fc_analysis(self,input_frame):\n",
    "        return 0\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_timesteps = 200\n",
    "conv1_analysis = compute_analysis(1, 8)\n",
    "conv2_analysis = compute_analysis(8, 16)\n",
    "conv3_analysis = compute_analysis(16, 20,(5,5))\n",
    "test_output_expected = np.zeros((test_size,1))\n",
    "test_output_array = np.zeros((test_size,number_of_timesteps))\n",
    "for batch_idx, (data, target) in enumerate(snn_test_loader):\n",
    "    start = batch_idx*train_batch_size\n",
    "    end = batch_idx*train_batch_size + data.shape[0]\n",
    "    data = data.cuda()\n",
    "    \n",
    "    spike_pot = torch.zeros_like(data).detach().cuda()\n",
    "    sp1_np = np.zeros((data.shape[0],8,26,26))\n",
    "    sp1_out = torch.tensor(sp1_np, requires_grad=False, dtype=torch.float32).cuda()\n",
    "    sp2_np = np.zeros((data.shape[0],16,24,24))\n",
    "    sp2_out = torch.tensor(sp2_np, requires_grad=False, dtype=torch.float32).cuda()\n",
    "    sp3_np = np.zeros((data.shape[0],20,8,8))\n",
    "    sp3_out = torch.tensor(sp3_np, requires_grad=False, dtype=torch.float32).cuda()\n",
    "    sp_fc1_np = np.zeros((data.shape[0],50))\n",
    "    sp_fc1_out = torch.tensor(sp_fc1_np, requires_grad=False, dtype=torch.float32).cuda()\n",
    "    sp_fc2_np = np.zeros((data.shape[0],10))\n",
    "    sp_fc2_out = torch.tensor(sp_fc2_np, requires_grad=False, dtype=torch.float32).cuda()\n",
    "    \n",
    "    for i in range(number_of_timesteps):\n",
    "        #print(i)\n",
    "        first_frame = True if i ==0 else False\n",
    "        spike_pot.add_(data)\n",
    "\n",
    "        spike_frame = torch.gt(spike_pot,1.0)*1.0 #F.threshold_(spike_pot,1,0).sign() #\n",
    "        spike_pot.sub_(spike_frame)\n",
    "        conv1_analysis.conv_analysis(spike_frame,first_frame)\n",
    "        sp1_out.add_(spikenet_1(spike_frame).detach())\n",
    "        sp2_in = torch.gt(sp1_out,1.0)*1.0 #F.threshold_(sp1_out, 1, 0) #\n",
    "        sp1_out.sub_(sp2_in)\n",
    "        conv2_analysis.conv_analysis(sp2_in,first_frame)\n",
    "\n",
    "        sp2_out.add_(spikenet_2(sp2_in).detach())\n",
    "        sp3_in = torch.gt(sp2_out,1.0)*1.0 #F.threshold_(sp2_out, 1, 0).sign() # \n",
    "        sp2_out.sub_(sp3_in)\n",
    "        conv3_analysis.conv_analysis(sp3_in,first_frame)\n",
    "\n",
    "        sp3_in_red = max_pool2(sp3_in)\n",
    "        sp3_out.add_(spikenet_3(sp3_in_red).detach())\n",
    "        sp4_in = torch.gt(sp3_out,1.0)*1.0 #F.threshold_(sp3_out, 1, 0).sign() #\n",
    "        sp3_out.sub_(sp4_in)\n",
    "        sp4_in_red = max_pool3(sp4_in)\n",
    "        sp4_in_red_flat = torch.flatten(sp4_in_red,start_dim=1)\n",
    "        sp_fc1_out.add_(spikenet_fc_1(sp4_in_red_flat).detach())\n",
    "        sp_fc2_in = torch.gt(sp_fc1_out,1.0)*1.0 #F.threshold_(sp_fc1_out, 1, 0).sign() #\n",
    "        sp_fc1_out.sub_(sp_fc2_in)\n",
    "        sp_fc2_out.add_(spikenet_fc_2(sp_fc2_in).detach())\n",
    "        sof_pot = F.softmax(sp_fc2_out).detach()\n",
    "        max_numbers = torch.argmax(sof_pot,dim=1)\n",
    "        print(torch.cuda.memory_allocated(device=0))\n",
    "        torch.cuda.empty_cache()\n",
    "    if(batch_idx==5):\n",
    "        break\n",
    "    else:\n",
    "        print('batch_id', batch_idx)\n",
    "\n",
    "\n",
    "\n",
    "print(conv1_analysis.summary())  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_cuda9p2",
   "language": "python",
   "name": "pytorch_cuda9p2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
